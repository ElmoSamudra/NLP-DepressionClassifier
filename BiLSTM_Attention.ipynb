{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Bidirectional, Embedding, Input\n",
    "from keras.layers import Flatten, Activation, RepeatVector, Permute, multiply, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../X_train.csv')\n",
    "X_test = pd.read_csv('../X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(X_train['tokenized_text']))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['tokenized_text'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_train = pad_sequences(list_tokenized_train, maxlen=150, padding='post')\n",
    "pad_test = pad_sequences(list_tokenized_test, maxlen=150, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = pd.get_dummies(X_train['mental_state']).values\n",
    "y_target_test = pd.get_dummies(X_test['mental_state']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector = {}\n",
    "f = open('glove.42B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size,300))\n",
    "# check if the word exist in tokenizer\n",
    "for word,i in tqdm(t.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    # some words may not exist in Glove\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "\n",
    "_input = Input(shape=(pad_train.shape[1],), dtype='int64')\n",
    "\n",
    "# get the embedding layer\n",
    "embedded = embeddings_layer(embeddings=embeddings_matrix,\n",
    "                            trainable=False, masking=False, scale=False, normalize=False)(_input)\n",
    "\n",
    "activations = LSTM(units, return_sequences=True)(embedded)\n",
    "\n",
    "# compute importance for each step\n",
    "attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(units)(attention)\n",
    "attention = Permute([2, 1])(attention)\n",
    "\n",
    "# apply the attention\n",
    "sent_representation = multiply([activations, attention])\n",
    "sent_representation = Lambda(lambda xin: backend.sum(xin, axis=0))(sent_representation)\n",
    "sent_representation = Flatten()(sent_representation)\n",
    "\n",
    "probabilities = Dense(3, activation='softmax')(sent_representation)\n",
    "\n",
    "model = Model(inputs=_input, outputs=probabilities)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
