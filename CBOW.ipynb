{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8df23474a8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# from spam\n",
    "from collections import Counter\n",
    "\n",
    "# word embedding\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n",
    "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../X_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1039139</th>\n",
       "      <td>[desperate, stay, hi, i, wan, na, start, ackno...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472691</th>\n",
       "      <td>[you, eat, moment, i, surviving, cereal, cold,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058630</th>\n",
       "      <td>[you, break, repetitive, cycle, i, generally, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181797</th>\n",
       "      <td>[fairly, quiet, weekend, my, son, his, friend,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073356</th>\n",
       "      <td>[white, trash, zilch, i, fat, white, trash, zi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokenized_text  target\n",
       "1039139  [desperate, stay, hi, i, wan, na, start, ackno...       1\n",
       "1472691  [you, eat, moment, i, surviving, cereal, cold,...       1\n",
       "1058630  [you, break, repetitive, cycle, i, generally, ...       1\n",
       "181797   [fairly, quiet, weekend, my, son, his, friend,...       0\n",
       "1073356  [white, trash, zilch, i, fat, white, trash, zi...       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(data['tokenized_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "emb = Word2Vec(sentences, size=EMBED_DIM, window=3, \n",
    "               min_count=3, negative=15, iter=1, \n",
    "               workers=multiprocessing.cpu_count())\n",
    "# get the word vector\n",
    "word_vec = emb.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=85971, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.save('../CBOW300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=130213, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('../CBOW300.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [200, 400, 600, 800, 1000]\n",
    "for dim in EMBED_DIMS:\n",
    "    emb = Word2Vec(sentences, size=dim, window=3, \n",
    "                   min_count=3, negative=15, iter=1, \n",
    "                   workers=multiprocessing.cpu_count())\n",
    "    emb.save(f'../CBOW{dim}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spam(data):\n",
    "    dic_counter = Counter(data)\n",
    "    if len(dic_counter)<=10 and len(data)>100:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'] = data['tokenized_text'].apply(lambda x: remove_spam(x))\n",
    "index_spam = data[data['spam']==0].index\n",
    "data.loc[list(index_spam), 'tokenized_text'] = data.loc[list(index_spam), \n",
    "                                                        'tokenized_text'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345318    [i, fucking, hate, you, i, fucking, hate, you,...\n",
       "1625692    [holy, fucking, shit, dude, get, me, get, me, ...\n",
       "1381157    [teach, me, speak, teach, me, share, teach, me...\n",
       "1536230    [i, care, post, deleted, express, my, feelings...\n",
       "1220478    [i, think, everything, universe, multiverse, f...\n",
       "1302575    [i, want, die, end, my, life, end, my, life, e...\n",
       "1352931    [i, hate, myself, much, i, wan, na, die, i, ha...\n",
       "782941     [days, till, new, top, gear, î, î, î, î, î, î,...\n",
       "1409221    [it, like, silent, mantra, i, ca, stop, i, wis...\n",
       "1345352    [stupid, bitch, stupid, fucking, ignorant, ung...\n",
       "1131699    [i, happgy, kidding, we, begin, i, hate, mysel...\n",
       "993573     [she, love, you, anymore, she, loves, someone,...\n",
       "1513776    [i, wan, na, die, i, wan, na, die, i, wan, na,...\n",
       "1187917    [i, wish, i, dead, i, wish, i, dead, i, wish, ...\n",
       "1481237    [nothing, ever, get, better, nothing, ever, ge...\n",
       "1607257    [my, little, poem, i, wish, i, dead, i, wish, ...\n",
       "1623245    [i, wish, my, brain, didnt, hate, me, i, wish,...\n",
       "941063     [i, want, die, i, idiot, i, hate, my, life, i,...\n",
       "1213262    [it, wont, give, i, gon, na, i, gon, na, i, go...\n",
       "1160266    [im, good, enough, im, good, enough, im, good,...\n",
       "968173     [i, want, die, life, suffering, life, sufferin...\n",
       "1505651    [im, breathing, im, fighting, im, breathing, i...\n",
       "1364167    [fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuc...\n",
       "1072111    [give, me, strength, accept, things, i, change...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[list(index_spam), 'tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'everything', 'fuck', 'i', 'knew', 'man', 'my', 'name', 'oh', 'wish', 'you'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.loc[1364167, 'tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>target</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1345318</th>\n",
       "      <td>[fucking, you, die, hope, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625692</th>\n",
       "      <td>[dude, me, fucking, get, holy, shit]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381157</th>\n",
       "      <td>[tell, me, go, share, aaaaaaaaaaaaaaaaaaaaaaaa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536230</th>\n",
       "      <td>[care, feelings, my, i, fuck, express, deleted...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220478</th>\n",
       "      <td>[think, universe, multiverse, everything, fuck...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302575</th>\n",
       "      <td>[life, my, die, end, i, want]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352931</th>\n",
       "      <td>[much, na, die, wan, i, hate, myself]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782941</th>\n",
       "      <td>[till, top, î, new, gear, days]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409221</th>\n",
       "      <td>[ca, dead, silent, like, wish, it, etc, mantra...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345352</th>\n",
       "      <td>[ignorant, fucking, ungrateful, cunt, stupid, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131699</th>\n",
       "      <td>[happgy, begin, myself, we, kidding, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993573</th>\n",
       "      <td>[else, loves, you, someone, she, anymore, love]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513776</th>\n",
       "      <td>[much, na, my, shit, die, wan, oh, god, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187917</th>\n",
       "      <td>[meaning, loses, dead, living, wish, hope, soo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481237</th>\n",
       "      <td>[ever, horrible, change, know, get, feel, bett...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607257</th>\n",
       "      <td>[dead, fucking, my, wish, poem, little, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623245</th>\n",
       "      <td>[brain, didnt, me, my, wish, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941063</th>\n",
       "      <td>[life, my, idiot, die, hate, i, want]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213262</th>\n",
       "      <td>[wont, na, gon, give, it, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160266</th>\n",
       "      <td>[na, die, wan, good, enough, i, im]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968173</th>\n",
       "      <td>[life, suffering, die, sufferinglife, i, want]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505651</th>\n",
       "      <td>[breathing, fighting, myself, say, stuff, i, im]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364167</th>\n",
       "      <td>[you, my, name, everything, wish, knew, oh, fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072111</th>\n",
       "      <td>[change, me, things, accept, give, strength, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokenized_text  target  spam\n",
       "1345318                 [fucking, you, die, hope, hate, i]       1     0\n",
       "1625692               [dude, me, fucking, get, holy, shit]       1     0\n",
       "1381157  [tell, me, go, share, aaaaaaaaaaaaaaaaaaaaaaaa...       1     0\n",
       "1536230  [care, feelings, my, i, fuck, express, deleted...       1     0\n",
       "1220478  [think, universe, multiverse, everything, fuck...       1     0\n",
       "1302575                      [life, my, die, end, i, want]       1     0\n",
       "1352931              [much, na, die, wan, i, hate, myself]       1     0\n",
       "782941                     [till, top, î, new, gear, days]       0     0\n",
       "1409221  [ca, dead, silent, like, wish, it, etc, mantra...       1     0\n",
       "1345352  [ignorant, fucking, ungrateful, cunt, stupid, ...       1     0\n",
       "1131699      [happgy, begin, myself, we, kidding, hate, i]       1     0\n",
       "993573     [else, loves, you, someone, she, anymore, love]       1     0\n",
       "1513776   [much, na, my, shit, die, wan, oh, god, hate, i]       1     0\n",
       "1187917  [meaning, loses, dead, living, wish, hope, soo...       1     0\n",
       "1481237  [ever, horrible, change, know, get, feel, bett...       1     0\n",
       "1607257         [dead, fucking, my, wish, poem, little, i]       1     0\n",
       "1623245              [brain, didnt, me, my, wish, hate, i]       1     0\n",
       "941063               [life, my, idiot, die, hate, i, want]       1     0\n",
       "1213262                       [wont, na, gon, give, it, i]       1     0\n",
       "1160266                [na, die, wan, good, enough, i, im]       1     0\n",
       "968173      [life, suffering, die, sufferinglife, i, want]       1     0\n",
       "1505651   [breathing, fighting, myself, say, stuff, i, im]       1     0\n",
       "1364167  [you, my, name, everything, wish, knew, oh, fu...       1     0\n",
       "1072111    [change, me, things, accept, give, strength, i]       1     0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['spam']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n"
     ]
    }
   ],
   "source": [
    "# GET MAX LEN IN INPUT\n",
    "len_sent = data['tokenized_text'].apply(lambda x: len(x))\n",
    "print(max(len_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1168251, 1131836,  807483, 1125164, 1348083, 1048175, 1312185,\n",
       "            1041726, 1634451, 1362631,  834659],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_sent = len_sent[len_sent>4000].index\n",
    "max_word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(data['target'])\n",
    "X = data[['tokenized_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace = True)\n",
    "y.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(labels='index', axis=1, inplace = True)\n",
    "y.drop(labels='index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/danielsafai/cnn-implementation-of-yoon-kim-s-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedShuffleSplit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-0f6bdaa993c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedShuffleSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StratifiedShuffleSplit' is not defined"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_dev = X.loc[train_index, :], X.loc[test_index, :]\n",
    "    y_train, y_dev = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "EMBED_SIZE = 300\n",
    "MAX_WORDS = 85973\n",
    "MAX_WORDS_IN_SENT = 4640\n",
    "\n",
    "list_sentences_train = data[\"comment_text\"].fillna(\"_NaN_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_NaN_\").values\n",
    "\n",
    "\n",
    "\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post')\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(X_train.shape[1],), dtype='int64')\n",
    "emb = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\n",
    "btch1_1 = BatchNormalization()(conv1_1)\n",
    "drp1_1  = Dropout(0.2)(btch1_1)\n",
    "actv1_1 = Activation('relu')(drp1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(actv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\n",
    "btch1_2 = BatchNormalization()(conv1_2)\n",
    "drp1_2  = Dropout(0.2)(btch1_2)\n",
    "actv1_2 = Activation('relu')(drp1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(actv1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=5)(emb)\n",
    "btch1_3 = BatchNormalization()(conv1_3)\n",
    "drp1_3  = Dropout(0.2)(btch1_3)\n",
    "actv1_3 = Activation('relu')(drp1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(actv1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\n",
    "drp1 = Dropout(0.2)(cnct)\n",
    "\n",
    "dns1  = Dense(32, activation='relu')(drp1)\n",
    "btch1 = BatchNormalization()(dns1)\n",
    "drp2  = Dropout(0.2)(btch1)\n",
    "\n",
    "out = Dense(y.shape[1], activation='sigmoid')(drp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenized_text    0\n",
       "target            0\n",
       "spam              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
