{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# from spam\n",
    "from collections import Counter\n",
    "\n",
    "# word embedding\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n",
    "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../X_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1039139</th>\n",
       "      <td>[desperate, stay, hi, i, wan, na, start, ackno...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472691</th>\n",
       "      <td>[you, eat, moment, i, surviving, cereal, cold,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058630</th>\n",
       "      <td>[you, break, repetitive, cycle, i, generally, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181797</th>\n",
       "      <td>[fairly, quiet, weekend, my, son, his, friend,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073356</th>\n",
       "      <td>[white, trash, zilch, i, fat, white, trash, zi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokenized_text  target\n",
       "1039139  [desperate, stay, hi, i, wan, na, start, ackno...       1\n",
       "1472691  [you, eat, moment, i, surviving, cereal, cold,...       1\n",
       "1058630  [you, break, repetitive, cycle, i, generally, ...       1\n",
       "181797   [fairly, quiet, weekend, my, son, his, friend,...       0\n",
       "1073356  [white, trash, zilch, i, fat, white, trash, zi...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(data['tokenized_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "emb = Word2Vec(sentences, size=EMBED_DIM, window=3, \n",
    "               min_count=3, negative=15, iter=1, \n",
    "               workers=multiprocessing.cpu_count())\n",
    "# get the word vector\n",
    "word_vec = emb.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=85971, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.save('../CBOW300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=85971, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('../CBOW300.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [200, 400, 600, 800, 1000]\n",
    "for dim in EMBED_DIMS:\n",
    "    emb = Word2Vec(sentences, size=dim, window=3, \n",
    "                   min_count=3, negative=15, iter=1, \n",
    "                   workers=multiprocessing.cpu_count())\n",
    "    emb.save(f'../CBOW{dim}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spam(data):\n",
    "    dic_counter = Counter(data)\n",
    "    if len(dic_counter)<=10 and len(data)>100:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'] = data['tokenized_text'].apply(lambda x: remove_spam(x))\n",
    "index_spam = data[data['spam']==0].index\n",
    "data.loc[list(index_spam), 'tokenized_text'] = data.loc[list(index_spam), \n",
    "                                                        'tokenized_text'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345318    [i, fucking, hate, you, i, fucking, hate, you,...\n",
       "1625692    [holy, fucking, shit, dude, get, me, get, me, ...\n",
       "1381157    [teach, me, speak, teach, me, share, teach, me...\n",
       "1536230    [i, care, post, deleted, express, my, feelings...\n",
       "1220478    [i, think, everything, universe, multiverse, f...\n",
       "1302575    [i, want, die, end, my, life, end, my, life, e...\n",
       "1352931    [i, hate, myself, much, i, wan, na, die, i, ha...\n",
       "782941     [days, till, new, top, gear, î, î, î, î, î, î,...\n",
       "1409221    [it, like, silent, mantra, i, ca, stop, i, wis...\n",
       "1345352    [stupid, bitch, stupid, fucking, ignorant, ung...\n",
       "1131699    [i, happgy, kidding, we, begin, i, hate, mysel...\n",
       "993573     [she, love, you, anymore, she, loves, someone,...\n",
       "1513776    [i, wan, na, die, i, wan, na, die, i, wan, na,...\n",
       "1187917    [i, wish, i, dead, i, wish, i, dead, i, wish, ...\n",
       "1481237    [nothing, ever, get, better, nothing, ever, ge...\n",
       "1607257    [my, little, poem, i, wish, i, dead, i, wish, ...\n",
       "1623245    [i, wish, my, brain, didnt, hate, me, i, wish,...\n",
       "941063     [i, want, die, i, idiot, i, hate, my, life, i,...\n",
       "1213262    [it, wont, give, i, gon, na, i, gon, na, i, go...\n",
       "1160266    [im, good, enough, im, good, enough, im, good,...\n",
       "968173     [i, want, die, life, suffering, life, sufferin...\n",
       "1505651    [im, breathing, im, fighting, im, breathing, i...\n",
       "1364167    [fuck, fuck, fuck, fuck, fuck, fuck, fuck, fuc...\n",
       "1072111    [give, me, strength, accept, things, i, change...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[list(index_spam), 'tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'everything', 'fuck', 'i', 'knew', 'man', 'my', 'name', 'oh', 'wish', 'you'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.loc[1364167, 'tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>target</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1345318</th>\n",
       "      <td>[fucking, you, die, hope, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625692</th>\n",
       "      <td>[dude, me, fucking, get, holy, shit]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381157</th>\n",
       "      <td>[tell, me, go, share, aaaaaaaaaaaaaaaaaaaaaaaa...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536230</th>\n",
       "      <td>[care, feelings, my, i, fuck, express, deleted...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220478</th>\n",
       "      <td>[think, universe, multiverse, everything, fuck...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302575</th>\n",
       "      <td>[life, my, die, end, i, want]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352931</th>\n",
       "      <td>[much, na, die, wan, i, hate, myself]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782941</th>\n",
       "      <td>[till, top, î, new, gear, days]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409221</th>\n",
       "      <td>[ca, dead, silent, like, wish, it, etc, mantra...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345352</th>\n",
       "      <td>[ignorant, fucking, ungrateful, cunt, stupid, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131699</th>\n",
       "      <td>[happgy, begin, myself, we, kidding, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993573</th>\n",
       "      <td>[else, loves, you, someone, she, anymore, love]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513776</th>\n",
       "      <td>[much, na, my, shit, die, wan, oh, god, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187917</th>\n",
       "      <td>[meaning, loses, dead, living, wish, hope, soo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481237</th>\n",
       "      <td>[ever, horrible, change, know, get, feel, bett...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607257</th>\n",
       "      <td>[dead, fucking, my, wish, poem, little, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623245</th>\n",
       "      <td>[brain, didnt, me, my, wish, hate, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941063</th>\n",
       "      <td>[life, my, idiot, die, hate, i, want]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213262</th>\n",
       "      <td>[wont, na, gon, give, it, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160266</th>\n",
       "      <td>[na, die, wan, good, enough, i, im]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968173</th>\n",
       "      <td>[life, suffering, die, sufferinglife, i, want]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505651</th>\n",
       "      <td>[breathing, fighting, myself, say, stuff, i, im]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364167</th>\n",
       "      <td>[you, my, name, everything, wish, knew, oh, fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072111</th>\n",
       "      <td>[change, me, things, accept, give, strength, i]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokenized_text  target  spam\n",
       "1345318                 [fucking, you, die, hope, hate, i]       1     0\n",
       "1625692               [dude, me, fucking, get, holy, shit]       1     0\n",
       "1381157  [tell, me, go, share, aaaaaaaaaaaaaaaaaaaaaaaa...       1     0\n",
       "1536230  [care, feelings, my, i, fuck, express, deleted...       1     0\n",
       "1220478  [think, universe, multiverse, everything, fuck...       1     0\n",
       "1302575                      [life, my, die, end, i, want]       1     0\n",
       "1352931              [much, na, die, wan, i, hate, myself]       1     0\n",
       "782941                     [till, top, î, new, gear, days]       0     0\n",
       "1409221  [ca, dead, silent, like, wish, it, etc, mantra...       1     0\n",
       "1345352  [ignorant, fucking, ungrateful, cunt, stupid, ...       1     0\n",
       "1131699      [happgy, begin, myself, we, kidding, hate, i]       1     0\n",
       "993573     [else, loves, you, someone, she, anymore, love]       1     0\n",
       "1513776   [much, na, my, shit, die, wan, oh, god, hate, i]       1     0\n",
       "1187917  [meaning, loses, dead, living, wish, hope, soo...       1     0\n",
       "1481237  [ever, horrible, change, know, get, feel, bett...       1     0\n",
       "1607257         [dead, fucking, my, wish, poem, little, i]       1     0\n",
       "1623245              [brain, didnt, me, my, wish, hate, i]       1     0\n",
       "941063               [life, my, idiot, die, hate, i, want]       1     0\n",
       "1213262                       [wont, na, gon, give, it, i]       1     0\n",
       "1160266                [na, die, wan, good, enough, i, im]       1     0\n",
       "968173      [life, suffering, die, sufferinglife, i, want]       1     0\n",
       "1505651   [breathing, fighting, myself, say, stuff, i, im]       1     0\n",
       "1364167  [you, my, name, everything, wish, knew, oh, fu...       1     0\n",
       "1072111    [change, me, things, accept, give, strength, i]       1     0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['spam']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n"
     ]
    }
   ],
   "source": [
    "# GET MAX LEN IN INPUT\n",
    "len_sent = data['tokenized_text'].apply(lambda x: len(x))\n",
    "print(max(len_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1168251, 1131836,  807483, 1125164, 1348083, 1048175, 1312185,\n",
       "            1041726, 1634451, 1362631,  834659],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_sent = len_sent[len_sent>4000].index\n",
    "max_word_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(data['target'])\n",
    "X = data[['tokenized_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace = True)\n",
    "y.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(labels='index', axis=1, inplace = True)\n",
    "y.drop(labels='index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/danielsafai/cnn-implementation-of-yoon-kim-s-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_dev = X.loc[train_index, :], X.loc[test_index, :]\n",
    "    y_train, y_dev = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = new_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x7fdd9c95e160>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vec.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['tokenized_text'].apply(lambda x: \" \".join(x))\n",
    "X_dev['text'] = X_dev['tokenized_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X_train = list(X_train['text'].values)\n",
    "list_X_dev = list(X_dev['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "EMBED_SIZE = 300\n",
    "MAX_WORDS = 85973\n",
    "MAX_WORDS_IN_SENT = 4640\n",
    "\n",
    "t = Tokenizer(num_words=MAX_WORDS)\n",
    "t.fit_on_texts(list_X_train)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "list_tokenized_train = t.texts_to_sequences(list_X_train)\n",
    "list_tokenized_test = t.texts_to_sequences(list_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(list_tokenized_train, maxlen=MAX_WORDS_IN_SENT, padding='post')\n",
    "X_test_pad = pad_sequences(list_tokenized_test, maxlen=MAX_WORDS_IN_SENT, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y_train['target']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for word in word_vec.vocab.keys():\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[word] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(X_train_pad.shape[1],), dtype='int64')\n",
    "emb = word_vec.get_keras_embedding()(inp)\n",
    "conv_filters = 100\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "#btch1_1 = BatchNormalization()(conv1_1)\n",
    "#drp1_1  = Dropout(0.2)(btch1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(conv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(emb)\n",
    "#btch1_2 = BatchNormalization()(conv1_2)\n",
    "#drp1_2  = Dropout(0.2)(btch1_2)\n",
    "#actv1_2 = Activation('relu')(drp1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(conv1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(emb)\n",
    "#btch1_3 = BatchNormalization()(conv1_3)\n",
    "#drp1_3  = Dropout(0.2)(btch1_3)\n",
    "#actv1_3 = Activation('relu')(drp1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(conv1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3], axis=1)\n",
    "#flatten = Flatten()(cnct)\n",
    "drp1 = Dropout(0.5)(cnct)\n",
    "\n",
    "dns1  = Dense(100, activation='relu')(drp1)\n",
    "#btch1 = BatchNormalization()(dns1)\n",
    "#drp2  = Dropout(0.2)(btch1)\n",
    "\n",
    "out = Dense(y.shape[1], activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 4640)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 4640, 300)    25791300    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 4638, 100)    90100       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 4637, 100)    120100      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 4636, 100)    150100      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 100)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 100)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 100)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 300)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            202         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 26,181,902\n",
      "Trainable params: 390,602\n",
      "Non-trainable params: 25,791,300\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(inputs=inp, outputs=out)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1064628 samples, validate on 118292 samples\n",
      "Epoch 1/1\n",
      "  10000/1064628 [..............................] - ETA: 45:54:55 - loss: 0.1766 - accuracy: 0.9308"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-c77d50c5bf5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(X_train_pad, y, validation_split=0.1, verbose=1, epochs=1, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
