{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import cumsum\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../X_train.pickle')\n",
    "X_test = pd.read_pickle('../X_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train['tokenized_text']))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['tokenized_text'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_train = pad_sequences(list_tokenized_train, maxlen=150, padding='post')\n",
    "pad_test = pad_sequences(list_tokenized_test, maxlen=150, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64\n",
    "max_length = 50\n",
    "\n",
    "_input = Input(shape=[max_length], dtype='int32')\n",
    "\n",
    "# get the embedding layer\n",
    "embedded = embeddings_layer(embeddings=embeddings_matrix,\n",
    "                            trainable=False, masking=False, scale=False, normalize=False)(_input)\n",
    "\n",
    "activations = LSTM(units, return_sequences=True)(embedded)\n",
    "\n",
    "# compute importance for each step\n",
    "attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
    "attention = Flatten()(attention)\n",
    "attention = Activation('softmax')(attention)\n",
    "attention = RepeatVector(units)(attention)\n",
    "attention = Permute([2, 1])(attention)\n",
    "\n",
    "# apply the attention\n",
    "sent_representation = multiply([activations, attention])\n",
    "sent_representation = Lambda(lambda xin: backend.sum(xin, axis=0))(sent_representation)\n",
    "sent_representation = Flatten()(sent_representation)\n",
    "\n",
    "probabilities = Dense(3, activation='softmax')(sent_representation)\n",
    "\n",
    "model = Model(inputs=_input, outputs=probabilities)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
