{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# from spam\n",
    "from collections import Counter\n",
    "\n",
    "# word embedding\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n",
    "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../X_train_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spam(data):\n",
    "    dic_counter = Counter(data)\n",
    "    if len(dic_counter)<=10 and len(data)>100:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'] = data['tokenized_text'].apply(lambda x: remove_spam(x))\n",
    "index_spam = data[data['spam']==0].index\n",
    "data.loc[list(index_spam), 'tokenized_text'] = data.loc[list(index_spam), \n",
    "                                                        'tokenized_text'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sent'] = data['tokenized_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('../X_train_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>target</th>\n",
       "      <th>spam</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>[i, love, u, guys, r, best]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i love u guys r best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>[im, meeting, one, my, besties, tonight, cant,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>im meeting one my besties tonight cant wait gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>[thanks, twitter, add, sunisa, i, got, meet, y...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks twitter add sunisa i got meet you hin s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>[sick, really, cheap, it, hurts, much, eat, re...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>sick really cheap it hurts much eat real food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>[he, effect, everyone]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>he effect everyone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenized_text  target  spam  \\\n",
       "800000                        [i, love, u, guys, r, best]       0     1   \n",
       "800001  [im, meeting, one, my, besties, tonight, cant,...       0     1   \n",
       "800002  [thanks, twitter, add, sunisa, i, got, meet, y...       0     1   \n",
       "800003  [sick, really, cheap, it, hurts, much, eat, re...       0     1   \n",
       "800004                             [he, effect, everyone]       0     1   \n",
       "\n",
       "                                                     sent  \n",
       "800000                               i love u guys r best  \n",
       "800001  im meeting one my besties tonight cant wait gi...  \n",
       "800002  thanks twitter add sunisa i got meet you hin s...  \n",
       "800003  sick really cheap it hurts much eat real food ...  \n",
       "800004                                 he effect everyone  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(data['tokenized_text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word embedding using CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "emb = Word2Vec(sentences, size=EMBED_DIM, window=3, \n",
    "               min_count=3, negative=15, iter=1, \n",
    "               workers=multiprocessing.cpu_count())\n",
    "# get the word vector\n",
    "word_vec = emb.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=95581, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.save('../CBOW300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=95581, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('../CBOW300.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [200, 400, 600, 800, 1000]\n",
    "for dim in EMBED_DIMS:\n",
    "    emb = Word2Vec(sentences, size=dim, window=3, \n",
    "                   min_count=3, negative=15, iter=1, \n",
    "                   workers=multiprocessing.cpu_count())\n",
    "    emb.save(f'../CBOW{dim}.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word embedding using skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "emb = Word2Vec(sentences, size=EMBED_DIM, window=3, \n",
    "               min_count=3, negative=15, iter=1, sg=1, \n",
    "               workers=multiprocessing.cpu_count())\n",
    "# get the word vector\n",
    "word_vec = emb.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.save('../SKIP-GRAM300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MAX LEN IN INPUT\n",
    "len_sent = data['tokenized_text'].apply(lambda x: len(x))\n",
    "print(max(len_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_sent = len_sent[len_sent>4000].index\n",
    "max_word_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train to train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(data['target'])\n",
    "X = data[['tokenized_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace = True)\n",
    "y.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(labels='index', axis=1, inplace = True)\n",
    "y.drop(labels='index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/danielsafai/cnn-implementation-of-yoon-kim-s-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_dev = X.loc[train_index, :], X.loc[test_index, :]\n",
    "    y_train, y_dev = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y_true, y_hat):\n",
    "    \n",
    "    y_true = list(map(lambda x: np.argmax(x), y_true))\n",
    "    y_hat = list(map(lambda x: np.argmax(x), y_hat))\n",
    "    \n",
    "    print('-'*40)\n",
    "    # accuracy\n",
    "    print('Accuracy: ', accuracy_score(y_true,y_hat))\n",
    "    # confusion matrix\n",
    "    print('\\n')\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_true,y_hat))\n",
    "    print('\\n')\n",
    "    # precision score of the model \n",
    "    print('Precision: ', precision_score(y_true, y_hat))\n",
    "    # recall score of the model \n",
    "    print('Recall: ', recall_score(y_true, y_hat))\n",
    "    # area under the ROC curve\n",
    "    print('Area under ROC curve: ', roc_auc_score(y_true, y_hat))\n",
    "    print('-'*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the CNN Non-Static Kim Yoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = new_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['tokenized_text'].apply(lambda x: \" \".join(x))\n",
    "X_dev['text'] = X_dev['tokenized_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X_train = list(X_train['text'].values)\n",
    "list_X_dev = list(X_dev['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "EMBED_SIZE = 300\n",
    "MAX_WORDS = 85971\n",
    "#MAX_WORDS_IN_SENT = 4640\n",
    "SET_LIMIT_SENTENCE = 150\n",
    "\n",
    "t = Tokenizer(num_words=MAX_WORDS)\n",
    "t.fit_on_texts(list_X_train)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "list_tokenized_train = t.texts_to_sequences(list_X_train)\n",
    "list_tokenized_test = t.texts_to_sequences(list_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(list_tokenized_train, maxlen=SET_LIMIT_SENTENCE, padding='post')\n",
    "X_test_pad = pad_sequences(list_tokenized_test, maxlen=SET_LIMIT_SENTENCE, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dummy for y values\n",
    "y = pd.get_dummies(y_train['target']).values\n",
    "y_test = pd.get_dummies(y_dev['target']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_X_train_pad = np.concatenate((X_train_pad[:25000], X_train_pad[-25000:]))\n",
    "sliced_y = np.concatenate((y[:25000], y[-25000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        try:\n",
    "            weight_matrix[i] = embedding[word]\n",
    "        except:\n",
    "            pass\n",
    "    return weight_matrix\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(word_vec, t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(X_train_pad.shape[1],), dtype='int64')\n",
    "#emb = word_vec.get_keras_embedding()(inp)\n",
    "emb = Embedding(vocab_size, EMBED_SIZE, weights=[embedding_vectors])(inp)\n",
    "conv_filters = 100\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "#btch1_1 = BatchNormalization()(conv1_1)\n",
    "#drp1_1  = Dropout(0.2)(btch1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(conv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(emb)\n",
    "#btch1_2 = BatchNormalization()(conv1_2)\n",
    "#drp1_2  = Dropout(0.2)(btch1_2)\n",
    "#actv1_2 = Activation('relu')(drp1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(conv1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(emb)\n",
    "#btch1_3 = BatchNormalization()(conv1_3)\n",
    "#drp1_3  = Dropout(0.2)(btch1_3)\n",
    "#actv1_3 = Activation('relu')(drp1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(conv1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3], axis=1)\n",
    "#flatten = Flatten()(cnct)\n",
    "drp1 = Dropout(0.5)(cnct)\n",
    "\n",
    "dns1  = Dense(100, activation='relu')(drp1)\n",
    "#btch1 = BatchNormalization()(dns1)\n",
    "#drp2  = Dropout(0.2)(btch1)\n",
    "\n",
    "out = Dense(y.shape[1], activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     83672700    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 148, 100)     90100       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 147, 100)     120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 146, 100)     150100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 100)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 300)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            202         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 84,063,302\n",
      "Trainable params: 84,063,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(inputs=inp, outputs=out)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 1161s 26ms/step - loss: 0.1065 - accuracy: 0.9613 - val_loss: 0.0530 - val_accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(sliced_X_train_pad, sliced_y, validation_split=0.1, verbose=1, epochs=1, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict model using validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_1.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99886513e-01, 1.13512004e-04],\n",
       "       [6.58603339e-03, 9.93413985e-01],\n",
       "       [9.99992490e-01, 7.46752630e-06],\n",
       "       ...,\n",
       "       [9.99996066e-01, 3.89573006e-06],\n",
       "       [9.27640258e-08, 9.99999881e-01],\n",
       "       [9.99997020e-01, 3.00331840e-06]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Accuracy:  0.9804236337902174\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[76932  2211]\n",
      " [  984 83080]]\n",
      "\n",
      "\n",
      "Precision:  0.974076983503535\n",
      "Recall:  0.9882946326608298\n",
      "Area under ROC curve:  0.980178929991762\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "performance(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data on twitter\n",
    "data_test = pd.read_pickle('../twitter_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[['tokenized_text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['text'] = data_test['tokenized_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X_twit_test = list(data_test['text'].values)\n",
    "list_tokenized_twit_test = t.texts_to_sequences(list_X_twit_test)\n",
    "X_twit_test = pad_sequences(list_tokenized_twit_test, maxlen=SET_LIMIT_SENTENCE, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_twit_test =  pd.get_dummies(data_test['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_twit = model_1.predict(X_twit_test)\n",
    "#accuracy_score(list(map(lambda x: np.argmax(x), y_twit_test)), list(map(lambda x: np.argmax(x), y_hat_twit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Accuracy:  0.9629593716668283\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[7765  235]\n",
      " [ 147 2166]]\n",
      "\n",
      "\n",
      "Precision:  0.9021241149521033\n",
      "Recall:  0.9364461738002594\n",
      "Area under ROC curve:  0.9535355869001296\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "performance(y_twit_test, y_hat_twit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the CNN Multi Channel Kim Yoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Channel ONE ##################################\n",
    "# channel 1 (CBOW)\n",
    "inputs1 = Input(shape=(X_train_pad.shape[1],))\n",
    "embedding1 = Embedding(vocab_size, EMBED_SIZE)(inputs1)\n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(embedding1)\n",
    "drop1_1 = Dropout(0.5)(conv1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(drop1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(embedding1)\n",
    "drop1_2 = Dropout(0.5)(conv1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(drop1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=6, activation='relu')(embedding1)\n",
    "drop1_3 = Dropout(0.5)(conv1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(drop1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct_1 = concatenate([glmp1_1, glmp1_2, glmp1_3], axis=1)\n",
    "drp1 = Dropout(0.5)(cnct_1)\n",
    "\n",
    "####################### Channel TWO ##################################\n",
    "# channel 2 (SKIP-GRAM)\n",
    "inputs2 = Input(shape=(X_train_pad.shape[1],))\n",
    "embedding2 = Embedding(vocab_size, EMBED_SIZE)(inputs2)\n",
    "conv2_1 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(embedding2)\n",
    "drop2_1 = Dropout(0.5)(conv2_1)\n",
    "glmp2_1 = GlobalMaxPooling1D()(drop2_1)\n",
    "\n",
    "conv2_2 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(embedding2)\n",
    "drop2_2 = Dropout(0.5)(conv2_2)\n",
    "glmp2_2 = GlobalMaxPooling1D()(drop2_2)\n",
    "\n",
    "conv2_3 = Conv1D(filters=conv_filters, kernel_size=6, activation='relu')(embedding2)\n",
    "drop2_3 = Dropout(0.5)(conv2_3)\n",
    "glmp2_3 = GlobalMaxPooling1D()(drop2_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct_2 = concatenate([glmp2_1, glmp2_2, glmp2_3], axis=1)\n",
    "drp2 = Dropout(0.5)(cnct_2)\n",
    "\n",
    "#################### Combined both channel ##############################\n",
    "# merge\n",
    "merged = concatenate([drp1, drp2])\n",
    "\n",
    "# interpretation\n",
    "dense1 = Dense(100, activation='relu')(merged)\n",
    "drp_last  = Dropout(0.2)(dense1)\n",
    "outputs = Dense(y.shape[1], activation='softmax')(drp_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 150, 300)     83672700    input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 150, 300)     83672700    input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 147, 100)     120100      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 146, 100)     150100      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 145, 100)     180100      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 147, 100)     120100      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 146, 100)     150100      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 145, 100)     180100      embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 147, 100)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 146, 100)     0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 145, 100)     0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 147, 100)     0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 146, 100)     0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 145, 100)     0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 100)          0           dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 100)          0           dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 100)          0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 100)          0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 100)          0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_30 (Global (None, 100)          0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 300)          0           global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "                                                                 global_max_pooling1d_27[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 300)          0           global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "                                                                 global_max_pooling1d_30[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 300)          0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 300)          0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 600)          0           dropout_38[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 100)          60100       concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 100)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            202         dropout_43[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 168,306,302\n",
      "Trainable params: 168,306,302\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_2 = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " 8256/45000 [====>.........................] - ETA: 20:42 - loss: 0.1386 - accuracy: 0.9490"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit([sliced_X_train_pad,sliced_X_train_pad] , sliced_y, \n",
    "                        validation_split=0.1, verbose=1, \n",
    "                        epochs=1, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on extra validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_2.predict(X_test_pad)\n",
    "performance(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_twit = model_1.predict(X_twit_test)\n",
    "performance(y_twit_test, y_hat_twit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
