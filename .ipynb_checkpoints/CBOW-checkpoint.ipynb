{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student.unimelb.edu.au/nyoewono/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# from spam\n",
    "from collections import Counter\n",
    "\n",
    "# word embedding\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.layers import Dense, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\n",
    "from keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../X_train_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spam(data):\n",
    "    dic_counter = Counter(data)\n",
    "    if len(dic_counter)<=10 and len(data)>100:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['spam'] = data['tokenized_text'].apply(lambda x: remove_spam(x))\n",
    "index_spam = data[data['spam']==0].index\n",
    "data.loc[list(index_spam), 'tokenized_text'] = data.loc[list(index_spam), \n",
    "                                                        'tokenized_text'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sent'] = data['tokenized_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('../X_train_temp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>target</th>\n",
       "      <th>spam</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>[i, love, u, guys, r, best]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>i love u guys r best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>[im, meeting, one, my, besties, tonight, cant,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>im meeting one my besties tonight cant wait gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>[thanks, twitter, add, sunisa, i, got, meet, y...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks twitter add sunisa i got meet you hin s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>[sick, really, cheap, it, hurts, much, eat, re...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>sick really cheap it hurts much eat real food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>[he, effect, everyone]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>he effect everyone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tokenized_text  target  spam  \\\n",
       "800000                        [i, love, u, guys, r, best]       0     1   \n",
       "800001  [im, meeting, one, my, besties, tonight, cant,...       0     1   \n",
       "800002  [thanks, twitter, add, sunisa, i, got, meet, y...       0     1   \n",
       "800003  [sick, really, cheap, it, hurts, much, eat, re...       0     1   \n",
       "800004                             [he, effect, everyone]       0     1   \n",
       "\n",
       "                                                     sent  \n",
       "800000                               i love u guys r best  \n",
       "800001  im meeting one my besties tonight cant wait gi...  \n",
       "800002  thanks twitter add sunisa i got meet you hin s...  \n",
       "800003  sick really cheap it hurts much eat real food ...  \n",
       "800004                                 he effect everyone  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(data['tokenized_text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word embedding using CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "emb = Word2Vec(sentences, size=EMBED_DIM, window=3, \n",
    "               min_count=3, negative=15, iter=1, \n",
    "               workers=multiprocessing.cpu_count())\n",
    "# get the word vector\n",
    "word_vec = emb.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.save('../CBOW300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('../CBOW300.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIMS = [200, 400, 600, 800, 1000]\n",
    "for dim in EMBED_DIMS:\n",
    "    emb = Word2Vec(sentences, size=dim, window=3, \n",
    "                   min_count=3, negative=15, iter=1, \n",
    "                   workers=multiprocessing.cpu_count())\n",
    "    emb.save(f'../CBOW{dim}.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word embedding using skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "emb = Word2Vec(sentences, size=EMBED_DIM, window=3, \n",
    "               min_count=3, negative=15, iter=1, sg=1, \n",
    "               workers=multiprocessing.cpu_count())\n",
    "# get the word vector\n",
    "word_vec = emb.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.save('../SKIP-GRAM300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET MAX LEN IN INPUT\n",
    "len_sent = data['tokenized_text'].apply(lambda x: len(x))\n",
    "print(max(len_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_sent = len_sent[len_sent>4000].index\n",
    "max_word_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the train to train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(data['target'])\n",
    "X = data[['tokenized_text']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.reset_index(inplace = True)\n",
    "y.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(labels='index', axis=1, inplace = True)\n",
    "y.drop(labels='index', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/danielsafai/cnn-implementation-of-yoon-kim-s-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
    "sss.get_n_splits(X, y)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_dev = X.loc[train_index, :], X.loc[test_index, :]\n",
    "    y_train, y_dev = y.loc[train_index], y.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y_true, y_hat):\n",
    "    \n",
    "    y_true = list(map(lambda x: np.argmax(x), y_true))\n",
    "    y_hat = list(map(lambda x: np.argmax(x), y_hat))\n",
    "    \n",
    "    print('-'*40)\n",
    "    # accuracy\n",
    "    print('Accuracy: ', accuracy_score(y_true,y_hat))\n",
    "    # confusion matrix\n",
    "    print('\\n')\n",
    "    print('Confusion Matrix: \\n', confusion_matrix(y_true,y_hat))\n",
    "    print('\\n')\n",
    "    # precision score of the model \n",
    "    print('Precision: ', precision_score(y_true, y_hat))\n",
    "    # recall score of the model \n",
    "    print('Recall: ', recall_score(y_true, y_hat))\n",
    "    # area under the ROC curve\n",
    "    print('Area under ROC curve: ', roc_auc_score(y_true, y_hat))\n",
    "    print('-'*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the CNN Non-Static Kim Yoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = new_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['tokenized_text'].apply(lambda x: \" \".join(x))\n",
    "X_dev['text'] = X_dev['tokenized_text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X_train = list(X_train['text'].values)\n",
    "list_X_dev = list(X_dev['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters\n",
    "EMBED_SIZE = 300\n",
    "MAX_WORDS = 85971\n",
    "#MAX_WORDS_IN_SENT = 4640\n",
    "SET_LIMIT_SENTENCE = 150\n",
    "\n",
    "t = Tokenizer(num_words=MAX_WORDS)\n",
    "t.fit_on_texts(list_X_train)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "list_tokenized_train = t.texts_to_sequences(list_X_train)\n",
    "list_tokenized_test = t.texts_to_sequences(list_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(list_tokenized_train, maxlen=SET_LIMIT_SENTENCE, padding='post')\n",
    "X_test_pad = pad_sequences(list_tokenized_test, maxlen=SET_LIMIT_SENTENCE, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dummy for y values\n",
    "y = pd.get_dummies(y_train['target']).values\n",
    "y_test = pd.get_dummies(y_dev['target']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_X_train_pad = X_train_pad[:50000]\n",
    "sliced_y = y[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 300))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        try:\n",
    "            weight_matrix[i] = embedding[word]\n",
    "        except:\n",
    "            pass\n",
    "    return weight_matrix\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(word_vec, t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(X_train_pad.shape[1],), dtype='int64')\n",
    "#emb = word_vec.get_keras_embedding()(inp)\n",
    "emb = Embedding(vocab_size, EMBED_SIZE, weights=[embedding_vectors])(inp)\n",
    "conv_filters = 100\n",
    "\n",
    "# Specify each convolution layer and their kernel siz i.e. n-grams \n",
    "conv1_1 = Conv1D(filters=conv_filters, kernel_size=3, activation='relu')(emb)\n",
    "#btch1_1 = BatchNormalization()(conv1_1)\n",
    "#drp1_1  = Dropout(0.2)(btch1_1)\n",
    "glmp1_1 = GlobalMaxPooling1D()(conv1_1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(emb)\n",
    "#btch1_2 = BatchNormalization()(conv1_2)\n",
    "#drp1_2  = Dropout(0.2)(btch1_2)\n",
    "#actv1_2 = Activation('relu')(drp1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(conv1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(emb)\n",
    "#btch1_3 = BatchNormalization()(conv1_3)\n",
    "#drp1_3  = Dropout(0.2)(btch1_3)\n",
    "#actv1_3 = Activation('relu')(drp1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(conv1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3], axis=1)\n",
    "#flatten = Flatten()(cnct)\n",
    "drp1 = Dropout(0.5)(cnct)\n",
    "\n",
    "dns1  = Dense(100, activation='relu')(drp1)\n",
    "#btch1 = BatchNormalization()(dns1)\n",
    "#drp2  = Dropout(0.2)(btch1)\n",
    "\n",
    "out = Dense(y.shape[1], activation='softmax')(dns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 150, 300)     73851900    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 148, 100)     90100       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 147, 100)     120100      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 146, 100)     150100      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 100)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 100)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 100)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 300)          0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 300)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          30100       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            202         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 74,242,502\n",
      "Trainable params: 74,242,502\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model(inputs=inp, outputs=out)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 720s 16ms/step - loss: 0.0808 - accuracy: 0.9712 - val_loss: 0.0533 - val_accuracy: 0.9820\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit(sliced_X_train_pad, sliced_y, validation_split=0.1, verbose=1, epochs=1, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev model using the split validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_1.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99886513e-01, 1.13512004e-04],\n",
       "       [6.58603339e-03, 9.93413985e-01],\n",
       "       [9.99992490e-01, 7.46752630e-06],\n",
       "       ...,\n",
       "       [9.99996066e-01, 3.89573006e-06],\n",
       "       [9.27640258e-08, 9.99999881e-01],\n",
       "       [9.99997020e-01, 3.00331840e-06]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981565172403299"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on all twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data on twitter\n",
    "data_test = pd.read_csv('sentiment_tweets3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10308</th>\n",
       "      <td>Many sufferers of depression aren't sad; they ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10309</th>\n",
       "      <td>No Depression by G Herbo is my mood from now o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10310</th>\n",
       "      <td>What do you do when depression succumbs the br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10311</th>\n",
       "      <td>Ketamine Nasal Spray Shows Promise Against Dep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10312</th>\n",
       "      <td>dont mistake a bad day with depression! everyo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10313 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message  label\n",
       "0      just had a real good moment. i missssssssss hi...      0\n",
       "1             is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2      @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3      @lapcat Need to send 'em to my accountant tomo...      0\n",
       "4          ADD ME ON MYSPACE!!!  myspace.com/LookThunder      0\n",
       "...                                                  ...    ...\n",
       "10308  Many sufferers of depression aren't sad; they ...      1\n",
       "10309  No Depression by G Herbo is my mood from now o...      1\n",
       "10310  What do you do when depression succumbs the br...      1\n",
       "10311  Ketamine Nasal Spray Shows Promise Against Dep...      1\n",
       "10312  dont mistake a bad day with depression! everyo...      1\n",
       "\n",
       "[10313 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = data_test[['message', 'label']]\n",
    "data_test.drop(10313)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_X_twit_test = list(data_test['message'].values)\n",
    "list_tokenized_twit_test = t.texts_to_sequences(list_X_twit_test)\n",
    "X_twit_test = pad_sequences(list_tokenized_twit_test, maxlen=SET_LIMIT_SENTENCE, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_twit_test =  pd.get_dummies(data_test['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9623812293969362"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_twit = model_1.predict(X_twit_test)\n",
    "accuracy_score(list(map(lambda x: np.argmax(x), y_twit_test)), list(map(lambda x: np.argmax(x), y_hat_twit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Accuracy:  0.9623812293969362\n",
      "\n",
      "\n",
      "Confusion Matrix: \n",
      " [[7740  260]\n",
      " [ 128 2186]]\n",
      "\n",
      "\n",
      "Precision:  0.8937040065412919\n",
      "Recall:  0.9446845289541919\n",
      "Area under ROC curve:  0.9560922644770959\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "performance(y_twit_test, y_hat_twit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design the CNN Multi Channel Kim Yoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Channel ONE ##################################\n",
    "# channel 1 (CBOW)\n",
    "inputs1 = Input(shape=(X_train_pad.shape[1],))\n",
    "embedding1 = Embedding(vocab_size, EMBED_SIZE)(inputs1)\n",
    "conv1 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(embedding1)\n",
    "drop1 = Dropout(0.5)(conv1)\n",
    "pool1 = GlobalMaxPooling1D()(drop1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(embedding1)\n",
    "drop1_2 = Dropout(0.5)(conv1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(drop1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=6, activation='relu')(embedding1)\n",
    "drop1_3 = Dropout(0.5)(conv1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(drop1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3], axis=1)\n",
    "\n",
    "####################### Channel TWO ##################################\n",
    "# channel 2 (SKIP-GRAM)\n",
    "inputs1 = Input(shape=(X_train_pad.shape[1],))\n",
    "embedding1 = Embedding(vocab_size, EMBED_SIZE)(inputs1)\n",
    "conv1 = Conv1D(filters=conv_filters, kernel_size=4, activation='relu')(embedding1)\n",
    "drop1 = Dropout(0.5)(conv1)\n",
    "pool1 = GlobalMaxPooling1D()(drop1)\n",
    "\n",
    "conv1_2 = Conv1D(filters=conv_filters, kernel_size=5, activation='relu')(embedding1)\n",
    "drop1_2 = Dropout(0.5)(conv1_2)\n",
    "glmp1_2 = GlobalMaxPooling1D()(drop1_2)\n",
    "\n",
    "conv1_3 = Conv1D(filters=conv_filters, kernel_size=6, activation='relu')(embedding1)\n",
    "drop1_3 = Dropout(0.5)(conv1_3)\n",
    "glmp1_3 = GlobalMaxPooling1D()(drop1_3)\n",
    "\n",
    "# Gather all convolution layers\n",
    "cnct = concatenate([glmp1_1, glmp1_2, glmp1_3], axis=1)\n",
    "\n",
    "\n",
    "# merge\n",
    "merged = concatenate([flat1, flat2])\n",
    "# interpretation\n",
    "dense1 = Dense(10, activation='relu')(merged)\n",
    "outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# summarize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(inputs=inp, outputs=out)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
